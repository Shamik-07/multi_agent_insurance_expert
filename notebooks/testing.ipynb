{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80a43c4a-e36e-438e-a7ef-d6b6ed1e88e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:36:58.593607Z",
     "iopub.status.busy": "2025-05-25T11:36:58.592363Z",
     "iopub.status.idle": "2025-05-25T11:36:59.409700Z",
     "shell.execute_reply": "2025-05-25T11:36:59.408793Z",
     "shell.execute_reply.started": "2025-05-25T11:36:58.593527Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel, VisitWebpageTool, WikipediaSearchTool, FinalAnswerTool, PythonInterpreterTool\n",
    "# import nest_asyncio\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cec2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inference_models = !curl -s https://huggingface.co/api/models | jq \".[].id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b518d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1c9797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info = model_info(\"Qwen/Qwen2.5-VL-72B-Instruct\", expand=\"inference\")\n",
    "info.inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_model = InferenceClientModel(model_id=\"Qwen/Qwen2.5-VL-72B-Instruct\",bill_to=\"VitalNest\",\n",
    "                                 timeout=300,temperature=.1, max_tokens=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0c7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085aa7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image file to a base64 string.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64 encoded image string.\n",
    "    \"\"\"\n",
    "    \"\"\"Encodes a PIL image to a base64 string.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f5b9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAXIMUM of 4 images can be sent\n",
    "img_paths = [\n",
    "    Path(\n",
    "        \"../src/pages/optima-restore-revision.pdf/optima-restore-revision.pdf_page_42.png\"\n",
    "    ),\n",
    "    Path(\n",
    "        \"../src/pages/optima-restore-revision.pdf/optima-restore-revision.pdf_page_12.png\"\n",
    "    ),\n",
    "    Path(\n",
    "        \"../src/pages/optima-restore-revision.pdf/optima-restore-revision.pdf_page_1.png\"\n",
    "    ),\n",
    "    # Path(\n",
    "    #     \"../src/pages/optima-restore-revision.pdf/optima-restore-revision.pdf_page_43.png\"\n",
    "    # ),\n",
    "    # Path(\n",
    "    #     \"../src/pages/optima-restore-revision.pdf/optima-restore-revision.pdf_page_44.png\"\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73a61942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_images = [Image.open(fp=pth) for pth in img_paths]\n",
    "grouped_images = [encode_image_to_base64(pth) for pth in img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fa8f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"\"\"what critical illnesses are covered under optima restore?\n",
    "If the answer isn't in these documents, say you don't know.\"\"\"\n",
    "chat_template = [\n",
    "    {\n",
    "\"role\":\"system\", \"content\":\"\"\"You find answers from the relevant documents. Answer only \n",
    "from these documents. If answer isn't available return 'Question cannot be answered based\n",
    "on the documents provided.' \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            for image in grouped_images\n",
    "        ]\n",
    "        + [{\"type\": \"text\", \"text\": text_query}],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e6d672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The critical illnesses covered under Optima Restore include Cancer, Open Chest CABG, First Heart Attack, Kidney Failure, Major Organ/Bone Marrow Transplant, Multiple Sclerosis, Permanent Paralysis of Limbs, and Stroke.\n"
     ]
    }
   ],
   "source": [
    "print(vlm_model.generate(messages=chat_template).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3766f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hyperbolic\",\n",
    "    bill_to=\"VitalNest\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f0477f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The critical illnesses covered under Optima Restore include Cancer, Open Chest CABG, First Heart Attack, Kidney Failure, Major Organ/Bone Marrow Transplant, Multiple Sclerosis, Permanent Paralysis of Limbs, and Stroke.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "    messages=chat_template,\n",
    "    temperature=.1,\n",
    "    max_tokens=10_000\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"text\", \"text\": \"Describe the images in one sentence.\"},\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\n",
    "#                         \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    "#                     },\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\n",
    "#                         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/3d/Nicu%C8%99or_Dan_%288_mai_2025%29.jpg\"\n",
    "#                     },\n",
    "#                 },\n",
    "#             ],\n",
    "#         }\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ce44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a5c91b2-c3dc-4a38-8d12-23ebde2324d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:36:59.411663Z",
     "iopub.status.busy": "2025-05-25T11:36:59.410797Z",
     "iopub.status.idle": "2025-05-25T11:36:59.419560Z",
     "shell.execute_reply": "2025-05-25T11:36:59.418230Z",
     "shell.execute_reply.started": "2025-05-25T11:36:59.411622Z"
    }
   },
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "class SaveContent(Tool):\n",
    "    name = \"SaveContent\"\n",
    "    description = \"Saves content to a file.\"\n",
    "    inputs = {\n",
    "        \"content\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The content to save.\"\n",
    "        },\n",
    "        \"filename\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The filename.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, content: str, filename: str ):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(content)\n",
    "        return f\"Content saved to {filename}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33cbf4-7ff8-4788-b595-9d73cbf1a2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:23:00.453929Z",
     "iopub.status.busy": "2025-05-25T11:23:00.452390Z",
     "iopub.status.idle": "2025-05-25T11:23:00.521867Z",
     "shell.execute_reply": "2025-05-25T11:23:00.520382Z",
     "shell.execute_reply.started": "2025-05-25T11:23:00.453816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool(), VisitWebpageTool(), \n",
    "                         PythonInterpreterTool(), WikipediaSearchTool(), FinalAnswerTool()], \n",
    "                  model=InferenceClientModel(bill_to=\"VitalNest\"),\n",
    "                  additional_authorized_imports=[\"os\", \"requests\", \"bs4\"],\n",
    "                 max_steps=10, planning_interval=2,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63367b-8496-44d6-981e-242733f59450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:23:01.188679Z",
     "iopub.status.busy": "2025-05-25T11:23:01.186566Z",
     "iopub.status.idle": "2025-05-25T11:24:33.318639Z",
     "shell.execute_reply": "2025-05-25T11:24:33.316639Z",
     "shell.execute_reply.started": "2025-05-25T11:23:01.188593Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.run(f\"\"\"Search for multi latent attention in wikipedia.\n",
    "If you cannot find a wikipedia page for it, search the entire wikipedia site for it.\n",
    "          Retrieve the top search results and the relevant content.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffb075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72d2e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import ToolCallingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a0c36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calling_agent = ToolCallingAgent(model=InferenceClientModel(model_id=\"Qwen/Qwen3-30B-A3B\", bill_to=\"VitalNest\"),\n",
    "                 tools=[DuckDuckGoSearchTool()], verbosity_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31461ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = tool_calling_agent.run(\"\"\"Search for `topic`:multi latent attention in wikipedia.\n",
    "If you cannot find a wikipedia page for it, search the entire wikipedia site for it.\n",
    "Look at the search results, if none of them contain information about the topic, \n",
    "                             then perform a normal search and return a summary of the retrieved content\n",
    "                             along with the search result links.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5bada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No Wikipedia page found for 'multi latent attention.' However, 'Multi-Head Latent Attention (MLA)' is a related concept discussed in articles like 'Attention Evolved' and 'TransMLA,' focusing on optimizing LLMs with low-rank compression.\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116b0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user wants me to search for \"multi latent attention\" on Wikipedia. Let me start by checking if there\\'s a direct page for that term. I\\'ll try the standard Wikipedia search first.\\n\\nHmm, when I search \"multi latent attention\" on Wikipedia, there\\'s no specific page dedicated to that exact term. The results might be empty or not relevant. Since the user mentioned that if there\\'s no page, I should search the entire Wikipedia site. But even after that, the top results from the web search (like the Medium articles, arXiv papers, and tech blogs) don\\'t point to a Wikipedia page. \\n\\nLooking at the sources provided, most of them are from external websites, not Wikipedia. The user\\'s task was to check Wikipedia first, then the entire site if needed. But since the web search results don\\'t include Wikipedia pages, it\\'s likely that there\\'s no official page on Wikipedia about \"multi latent attention.\" \\n\\nThe user also wants the top search results and relevant content. The top results from the web include mentions of Multi-Head Latent Attention (MLA) in contexts like DeepSeek\\'s research, arXiv papers, and technical blogs. These sources explain MLA as an attention mechanism in LLMs, focusing on reducing KV-cache size and computational complexity. However, none of these are from Wikipedia. \\n\\nSince the user specified to return only the final answer, I need to condense this information. The key points are: no Wikipedia page exists for \"multi latent attention,\" but related concepts like MLA are discussed in academic and technical sources. The answer should mention that while there\\'s no direct Wikipedia page, the top results refer to MLA as an optimization technique in LLMs. I should also note the key aspects like low-rank compression, KV-cache reduction, and sources like DeepSeek and arXiv.\\n</think>\\n\\nNo direct Wikipedia page found for \"multi latent attention.\" However, top results discuss **Multi-Head Latent Attention (MLA)** as an attention mechanism in large language models (LLMs), focusing on reducing key-value (KV) cache memory and computational complexity. Key details include:  \\n- Uses low-rank compression for key-value projections.  \\n- Optimizes memory efficiency in models like DeepSeek-V2.  \\n- Combines ideas from traditional attention mechanisms with latent space representations.  \\n\\nSources include research papers (e.g., arXiv), technical blogs, and industry analyses, but no official Wikipedia page exists for this term.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calling_agent.provide_final_answer(\"\"\"Search for multi latent attention in wikipedia.\n",
    "If you cannot find a wikipedia page for it, search the entire wikipedia site for it.\n",
    "          Retrieve the top search results and the relevant content.\n",
    "                       Return only the final answer.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5891393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6f5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba3965a0962432ea889e6de249c5bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3e897980b84d238b5cedc58586704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = ColQwen2_5.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"vidore/colqwen2.5-v0.2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n",
    "    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    ").eval()\n",
    "processor = ColQwen2_5_Processor.from_pretrained(\"vidore/colqwen2.5-v0.2\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc98854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your inputs\n",
    "images = [\n",
    "    Image.new(\"RGB\", (128, 128), color=\"white\"),\n",
    "    Image.new(\"RGB\", (128, 128), color=\"black\"),\n",
    "]\n",
    "queries = [\n",
    "    \"Is attention really all you need?\",\n",
    "    \"What is the amount of bananas farmed in Salvador?\",\n",
    "]\n",
    "\n",
    "# Process the inputs\n",
    "batch_images = processor.process_images(images).to(model.device)\n",
    "batch_queries = processor.process_queries(queries).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    image_embeddings = model(**batch_images)\n",
    "    query_embeddings = model(**batch_queries)\n",
    "\n",
    "scores = processor.score_multi_vector(query_embeddings, image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1ac9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
